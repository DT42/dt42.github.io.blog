<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Simple example codes for parallelizing neural network | DT42 Tech Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/3.0.3/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/2.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Simple example codes for parallelizing neural network</h1><a id="logo" href="/.">DT42 Tech Blog</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Simple example codes for parallelizing neural network</h1><div class="post-meta">Apr 27, 2016<script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-disqus-identifier="2016/04/27/simple-example-codes-for-parallelizing-neural-network/" href="/2016/04/27/simple-example-codes-for-parallelizing-neural-network/#disqus_thread" class="disqus-comment-count"></a><div class="post-content"><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>If you are doing deep learning, one of the most challenging tasks in recent years is to make it run faster.  A very common choice is to compute gradient descent in parallel on the distributed systems using either data parallelism or model parallelism.  This article is meant to demonstrate ways to implement basic parallelism concepts from scratch using a simple model.  It will also show how these parallelism techniques really improve the performance of training a neural network.  If you are looking for more materials about the parallelism techniques, I recommend two easy-to-understand articles written by Tim Dettmers [1][2].</p>
<p>This article focuses on the implementations of data parallelism using a very simple neural network model, so the readers only need to have the basic knowledge of neural network (such as what is neural network).  If you are not yet very familiar with deep learning and the mathematics behind, don’t worry, you can still get most of the ideas mentioned in this article.</p>
<p>The example codes are available on <a href="https://github.com/DT42/neural-network-model-manipulations" target="_blank" rel="external">https://github.com/DT42/neural-network-model-manipulations</a>.</p>
<h1 id="Setup-development-environment"><a href="#Setup-development-environment" class="headerlink" title="Setup development environment"></a>Setup development environment</h1><p>Note: If you are only interested in reading the example codes instead of making your hand (computer) dirty, you can jump to next section directly.</p>
<p>To execute the example codes, the necessary dependencies are listed below (my OS is Ubuntu desktop 14.04):</p>
<ol>
<li><code>virtualenv</code>: To create a clean testing environment.</li>
<li><code>numpy</code>: For matrix operations in CPU mode.</li>
<li><code>mpi4py</code>: To split and distribute a dataset to worknodes. </li>
<li><code>Theano</code>: For matrix operations in GPU mode.</li>
<li><code>CUDA</code>: To provide GPU computing infrastructure.</li>
</ol>
<p>Note: <code>scipy</code> and <code>six</code> will also be installed automatically because of the dependency condition.</p>
<h2 id="System-dependencies"><a href="#System-dependencies" class="headerlink" title="System dependencies"></a>System dependencies</h2><p>Before installing the dependencies listed above, the following system dependencies are required:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br><span class="line">$ sudo apt-get install python-pip python-dev libopenmpi-dev gfortran libblas-dev liblapack-dev</span><br></pre></td></tr></table></figure>
<h2 id="Dependencies-required-for-running-examples"><a href="#Dependencies-required-for-running-examples" class="headerlink" title="Dependencies required for running examples"></a>Dependencies required for running examples</h2><p>Now, we are ready to install the dependencies of the examples.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ virtualenv -p python2.7 nn-test</span><br><span class="line">$ cd nn-test</span><br><span class="line">$ source bin/activate</span><br><span class="line">(nn-test) $ pip install mpi4py numpy Theano</span><br></pre></td></tr></table></figure>
<p>To install CUDA 7.5, you just need to follow the instructions in CUDA toolkit documentation [3][4], or leverage Roelof’s installation script [5] which will install CUDA 7.0.</p>
<h1 id="Read-the-source-code-Luke"><a href="#Read-the-source-code-Luke" class="headerlink" title="Read the source code, Luke"></a>Read the source code, Luke</h1><p>This section consists of two parts. First, I will demonstrate how to write a simple neural network model to recognize numbers in the digit images from scratch using MNIST dataset.  Second, I will demonstrate how to improve performance by data parallelism.</p>
<p>To download the example codes:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https://github.com/DT42/neural-network-model-manipulations.git</span><br></pre></td></tr></table></figure>
<h2 id="Simple-neural-network"><a href="#Simple-neural-network" class="headerlink" title="Simple neural network"></a>Simple neural network</h2><p>The sample code of this simple neural network model is inspired by Andrew Ng’s machine learning course hosted on Coursera [6], and the simplified MNIST training data is copied from the neural network assignment.  If you are interested in the mathematical formulas used in the source code, I encourage you to take this MOOC course.</p>
<p><img src="https://docs.google.com/uc?id=0B6Zw6hselblgallieW11TnZtd0k" alt="Figure 1: Simple neural network structure"> </p>
<p>Figure 1 is the structure of the neural network used in the example <code>mnist-nn.py</code>.  Here is the pseudo code of the example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">trainingset, labels = load_dataset()</span><br><span class="line">model = train(trainingset, labels, learningrate, iteration)</span><br><span class="line">outputs = predict(model, inputs)</span><br><span class="line">precision = check_precision(outputs, labels)</span><br></pre></td></tr></table></figure>
<p>In the <code>train</code> function, the optimizer is batch gradient descent, and the activation function is sigmoid.</p>
<p>If you set learning rate to 2 and iteration to 1000, cost will be around 0.25 and precision will be around 97%.</p>
<h2 id="Parallelism-MPI-GPU"><a href="#Parallelism-MPI-GPU" class="headerlink" title="Parallelism: MPI + GPU"></a>Parallelism: MPI + GPU</h2><p>To reduce the training time, MPI and GPU computing technologies are used here to implement data parallelism.  The core concept is to split the input dataset, distribute the subsets to the worknodes, and collect the results computed by GPU.  The example of data parallelism with GPU computing is <code>mnist-nn-data-parallelism.py</code>.</p>
<p><img src="https://drive.google.com/uc?id=0B6Zw6hselblgTGxPUkJ2ZHNhSFk" alt="Figure 2: Concept of data parallelism on GPU cluster"></p>
<p>MPI standard defines tons of easy-to-use distributed operations [7].  Figure 3 shows some MPI collective functions used in the example:</p>
<p><img src="https://docs.google.com/uc?id=0B6Zw6hselblgSFNoVS1jVEgyalk" alt="Figure 3: Some of MPI collective functions (copied from MPI standard 3.0)"></p>
<p>In figure 3, the parallel processes can happen in the same machine or in a cluster.  The broadcast function helps a process share information with the other processes.  All the processes will get a copy of the data assigned by the broadcast function.  The scatter function splits a dataset into several subsets and makes the subsets evenly distributed to a group of processes, while the gather function does the opposite, collects data from a group of processes, and reassembles the collected data into a complete dataset.</p>
<p>Let’s see some code snippets in the example (modified slightly for readability):</p>
<ol>
<li><p>To distribute dataset to worknodes (<code>Scatter</code>).</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sliced_inputs = np.asarray(np.split(inputs, num_worknodes))</span><br><span class="line">sliced_labels = np.asarray(np.split(labels, num_worknodes))</span><br><span class="line">inputs_buf = np.zeros((len(inputs)/num_worknodes, Input_layer_size))</span><br><span class="line">labels_buf = np.zeros((len(labels)/num_worknodes))</span><br><span class="line">comm.Scatter(sliced_labels, labels_buf)</span><br></pre></td></tr></table></figure>
</li>
<li><p>To collect training results of worknodes (<code>Gather</code>).</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cost, (theta1_grad, theta2_grad) = cost_function(</span><br><span class="line">    theta1, theta2,</span><br><span class="line">    Input_layer_size,</span><br><span class="line">    Hidden_layer_size,</span><br><span class="line">    Output_layer_size,</span><br><span class="line">    inputs_buf, labels_buf,</span><br><span class="line">    regular=<span class="number">0</span>)</span><br><span class="line">theta1_grad_buf = np.asarray([np.zeros_like(theta1_grad)] * num_worknodes)</span><br><span class="line">comm.Gather(theta1_grad, theta1_grad_buf)</span><br><span class="line">theta1_grad = functools.reduce(np.add, theta1_grad_buf) / num_worknodes</span><br></pre></td></tr></table></figure>
</li>
<li><p>To synchronize the weights of neural network among worknodes (Bcast).</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">theta1 -= learningrate * theta1_grad</span><br><span class="line">comm.Bcast([theta1, MPI.DOUBLE])</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>Because there are a lot of matrix multiplications involved in the training process, It is very good to use GPU to speed up the computation.  Theano provides high level functions so you don’t need to worry about the details communicating with GPU.  Here is the source code using Theano to multiply two matrices:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gpu_matrix_dot</span><span class="params">()</span>:</span></span><br><span class="line">    x = T.matrix(<span class="string">'x'</span>)</span><br><span class="line">    y = T.matrix(<span class="string">'y'</span>)</span><br><span class="line">    z = T.dot(x, y)</span><br><span class="line">    f = theano.function(</span><br><span class="line">        [x, y], z)</span><br><span class="line">    <span class="keyword">return</span> f</span><br><span class="line">    </span><br><span class="line">Matrix_dot = gpu_matrix_dot()</span><br><span class="line">c = Matrix_dot(a, b)</span><br></pre></td></tr></table></figure>
<p>Now, you have seen the most essential concepts of data parallelism.  Here I would like to share more implementation tips:</p>
<ol>
<li>Compile a Theano function once, and use it repeatedly: It is time-consuming to compile a Theano function; your program can be very slow if the compilation is required every time when the  Theano function is called.</li>
<li>Check memory ordering of a numpy array carefully: In the examples, we get matrices by <code>scipy.io.loadmat</code>, but their memory ordering is Fortran-order instead of C-order [8].  If we broadcast a Fortran-order buffer without notifying this pitfall, we might create C-order buffers to the other worknodes, and they will receive unexpected values in their buffers. For more details, you can check the source codes in <code>tests/tests_broadcast-weights.py</code>.</li>
</ol>
<h2 id="Environment"><a href="#Environment" class="headerlink" title="Environment"></a>Environment</h2><p>A 2-worknode GPU cluster is used to measure the performance of data parallelism.  The hardware information of a worknode is:</p>
<ul>
<li>CPU: Intel i7-5930 3.5 GHz</li>
<li>system memory: 32 GB</li>
<li>network speed: 1 Gbps Ethernet interface</li>
<li>GPU: Nvidia Titan X</li>
</ul>
<p>To measure the network speed between two worknodes, <code>nc</code> is a convenient tool to create a TCP connection between a temporary client-server pair used to send testing data and get the connection speed [9]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># temporary server, IP is 192.168.1.1</span><br><span class="line">worknode1 $ nc -vvln 50001 &gt; /dev/null</span><br><span class="line">Listening on [0.0.0.0] (family 0, port 50001)</span><br><span class="line">Connection from [192.168.1.2] port 50001 [tcp/*] accepted (family 2, sport 41018)</span><br><span class="line"></span><br><span class="line"># temporary client</span><br><span class="line">worknode2 $ dd if=/dev/zero bs=1M count=1K | nc -vvn 192.168.1.1 50001</span><br><span class="line">Connection to 192.168.1.1 50001 port [tcp/*] succeeded!</span><br><span class="line">1024+0 records in</span><br><span class="line">1024+0 records out</span><br><span class="line">1073741824 bytes (1.1 GB) copied, 9.11171 s, 118 MB/s</span><br></pre></td></tr></table></figure>
<h2 id="Scenarios"><a href="#Scenarios" class="headerlink" title="Scenarios"></a>Scenarios</h2><p>Here are experiment scenarios:</p>
<ol>
<li>Data parallelism vs local execution: with CPU</li>
<li>Data parallelism vs local execution: with GPU</li>
</ol>
<p>The parameters of the learning algorithm:</p>
<ul>
<li>learning rate: 0.1</li>
<li>gradient descent iteration: 60</li>
</ul>
<h2 id="Experiment-results"><a href="#Experiment-results" class="headerlink" title="Experiment results"></a>Experiment results</h2><p>Figure {4,5,6} show the results of data parallelism performance comparison, and tells us two essential facts:</p>
<ol>
<li>Data parallelism brings performance improvement.</li>
<li>GPU computing seems to be slower than CPU computing, is it true?</li>
</ol>
<p><img src="https://drive.google.com/uc?id=0B6Zw6hselblgRkw4eEVjR2t2WDQ" alt="Figure 4: Data parallelism performance comparison"></p>
<p><img src="https://drive.google.com/uc?id=0B6Zw6hselblgSGdPWV9GMnRfSmc" alt="Figure 5: Performance comparison at forward propagation stage"></p>
<p><img src="https://drive.google.com/uc?id=0B6Zw6hselblgWjBqNGZMRzNFc00" alt="Figure 6: Performance comparison at back propagation stage"></p>
<p>At the first glance, the improvement seems to be trivial.  If the dataset is split and distributed evenly on two machines (so doubled computing units), of course the speed can also be two times faster than using a single machine alone.  However, the really important fact behind is that we are now able to scale-out the computing ability from one machine to the GPU cluster easily.  Since GPU computing is still one of the most effective technology for deep learning, this technique can make the learning process much faster.</p>
<p>Secondly, the fact that GPU is slower in those figures does not mean that CPU is a better tool than GPU for deep learning; it is also not a bug in the source code (lol).  Actually, it just reflects the fact that the cost of memory copy between system and GPU memories is expensive.  Because the size of the sample neural network is small, the computation loads are not really heavy.  Therefore, the improvement to the total computing time by using GPU is overwhelmed by the time consumed while copying inputs and outputs between system memory and GPU memory.</p>
<p>When the size of the data is bigger, the ratio of time to compute and time to execute memory copy is also getting larger; GPU computing starts to show its power.  Huge dataset and larger number of hidden layers make GPU computing a desirable tool for deep learning.  Let’s make a very simple experiment to test if our understanding about GPU performance is true.  In figure {7,8}, two matrix of different sizes are multiplied.  One can see that GPU is only slower than CPU when one of the matrix sizes is below 120x30 while the size of another one is below 30x20.</p>
<p><img src="https://drive.google.com/uc?id=0B6Zw6hselblgZGlQSE55SEN3OFk" alt="Figure 7: When the data is small, the total time consumed by GPU computing is longer than using CPU alone"></p>
<p><img src="https://drive.google.com/uc?id=0B6Zw6hselblgMXhfMExoM0sxeHc" alt="Figure 8: GPU computing is far faster than CPU computing on large matrices"></p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>For deep learning, GPU cluster is one of the most powerful tools to help reduce the computing time [10].  To overcome the hardware limitation of a single machine, we can also scale out the computing resource to make distributed systems.  Parallelism enables data scientists and engineers to propose more powerful solutions, and get experimental results sooner.</p>
<p>Besides of architecture design of GPU, there are also other interesting technologies which try to reduce the computation time, such as CUDA-Aware MPI and Nvidia GPUDirect [11], specific computing hardware (Teradeep [12], Singular Computing [13]).</p>
<p>Model compression is another interesting topic which reduces the computational loads from a different approach without dropping precision much [14][15].</p>
<p>Special thanks:</p>
<ul>
<li>Tammy Yang’s suggestions for grammatical mistakes.</li>
</ul>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol>
<li><a href="http://timdettmers.com/2014/10/09/deep-learning-data-parallelism/" target="_blank" rel="external">Tim Dettmers, How to Parallelize Deep Learning on GPUs Part 1/2: Data Parallelism, 2014</a></li>
<li><a href="http://timdettmers.com/2014/11/09/model-parallelism-deep-learning/" target="_blank" rel="external">Tim Dettmers, How to Parallelize Deep Learning on GPUs Part 2/2: Model Parallelism, 2014</a></li>
<li><a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="external">Nvidia, CUDA 7.5 downloads</a></li>
<li><a href="http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-linux/index.html#ubuntu-installation" target="_blank" rel="external">Nvidia, CUDA toolkit document</a></li>
<li><a href="http://graphific.github.io/posts/running-a-deep-learning-dream-machine/" target="_blank" rel="external">Roelof Pieters, II: Running a Deep Learning (Dream) Machine, 2015</a></li>
<li><a href="https://www.coursera.org/learn/machine-learning/" target="_blank" rel="external">Andrew Ng, Machine Learning, Coursera</a></li>
<li><a href="http://www.mpi-forum.org/docs/mpi-3.0/mpi30-report.pdf" target="_blank" rel="external">MPI Forum, MPI: A Message-Passing Interface Standard Version 3.0, 2012</a></li>
<li><a href="http://docs.scipy.org/doc/numpy/reference/internals.html#multidimensional-array-indexing-order-issues" target="_blank" rel="external">Numpy.org, Numpy internals</a></li>
<li><a href="http://askubuntu.com/questions/7976/how-do-you-test-the-network-speed-betwen-two-boxes" target="_blank" rel="external">How do you test the network speed between two boxes?</a></li>
<li><a href="http://jmlr.org/proceedings/papers/v28/coates13.pdf" target="_blank" rel="external">Coates, Adam, et al. “Deep learning with COTS HPC systems.” Proceedings of the 30th international conference on machine learning. 2013.</a></li>
<li><a href="https://devblogs.nvidia.com/parallelforall/introduction-cuda-aware-mpi/" target="_blank" rel="external">Jiri Kraus, An Introduction to CUDA-Aware MPI, 2013</a></li>
<li><a href="http://www.teradeep.com/" target="_blank" rel="external">Teradeep</a></li>
<li><a href="https://www.technologyreview.com/s/601263/why-a-chip-thats-bad-at-math-can-help-computers-tackle-harder-problems/" target="_blank" rel="external">Tom Simonite, Why a chip that’s bad at math can help computers tackle harder problems, MIT technology review, 2016</a></li>
<li><a href="http://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network.pdf" target="_blank" rel="external">Han, Song, et al. “Learning both Weights and Connections for Efficient Neural Network.” Advances in Neural Information Processing Systems. 2015.</a></li>
<li><a href="http://arxiv.org/pdf/1602.01528v1.pdf" target="_blank" rel="external">Han, Song, et al. “EIE: Efficient Inference Engine on Compressed Deep Neural Network.” arXiv preprint arXiv:1602.01528 (2016).</a></li>
</ol>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://dt42.github.io/2016/04/27/simple-example-codes-for-parallelizing-neural-network/" data-id="cisi945vn0001ezgqnbfe8sj5" class="article-share-link">Share</a><div class="tags"><a href="/tags/parallelism-deep-learning-english/">parallelism, deep-learning, english</a></div><div class="post-nav"><a href="/2016/05/09/deep-learning-machine-construction/" class="pre">打造自己的深度學習計算環境</a><a href="/2016/04/27/deep-learning-material-recommendations/" class="next">深度學習(Deep Learning)自學素材推薦</a></div><div id="disqus_thread"><script>var disqus_shortname = 'dt42';
var disqus_identifier = '2016/04/27/simple-example-codes-for-parallelizing-neural-network/';
var disqus_title = 'Simple example codes for parallelizing neural network';
var disqus_url = 'http://dt42.github.io/2016/04/27/simple-example-codes-for-parallelizing-neural-network/';
(function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//dt42.disqus.com/count.js" async></script></div></div></div></div><div class="pure-u-1-4"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://dt42.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/ubuntu-cuda-computer-building-chinese/" style="font-size: 15px;">ubuntu, cuda, computer-building, chinese</a> <a href="/tags/parallelism-deep-learning-english/" style="font-size: 15px;">parallelism, deep-learning, english</a> <a href="/tags/deep-learning-mandarin-chinese/" style="font-size: 15px;">deep-learning, mandarin-chinese</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2016/05/09/deep-learning-machine-construction/">打造自己的深度學習計算環境</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/04/27/simple-example-codes-for-parallelizing-neural-network/">Simple example codes for parallelizing neural network</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/04/27/deep-learning-material-recommendations/">深度學習(Deep Learning)自學素材推薦</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> Recent Comments</i></div><script type="text/javascript" src="//dt42.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Blogroll</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">DT42 Tech Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>